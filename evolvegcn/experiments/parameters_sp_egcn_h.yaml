data: sp #HELP: arxiv, bitcoin, aml_sim, dbg, elliptic, elliptic_temporal,sp
sp_args:
  edges_file: ./data/dataloader/sp/edges.pickle
  nodes_file: ./data/dataloader/sp/nodes.pickle
  nodes_feats_file: ./data/dataloader/sp/nodes_feats.pickle
  nodes_labels_times_file: ./data/dataloader/sp/nodes_labels_times-M.pickle
  aggr_time: 1

use_cuda: True
use_logfile: True

model: egcn_h # Choosen variant of the model (-H), considers hidden states in the updates. Not tested with the -O variant.

task: node_cls

class_weights: [ 1.0, 10.0] 
use_2_hot_node_feats: False
use_1_hot_node_feats: False
save_node_embeddings: False # still having problem with this

train_proportion: 0.75 #0.65
dev_proportion: 0.125
num_epochs: 2000
steps_accum_gradients: 1
learning_rate: 0.001
learning_rate_min: 0.001
learning_rate_max: 0.02
negative_mult_training: 20
negative_mult_test: 100
smart_neg_sampling: False
seed: 1234
target_measure: F1 # measure to define the best epoch F1, Precision, Recall, MRR, MAP
target_class: 1 # Target class to get the measure to define the best epoch (all, 0, 1)
early_stop_patience: 100

eval_after_epochs: 5
adj_mat_time_window: 1  # Time window to create the adj matrix for each timestep. Use None to use all the history (from 0 to t)
adj_mat_time_window_min: 1
adj_mat_time_window_max: 10
num_hist_steps: 5  #5 # number of previous steps used for prediction
num_hist_steps_min: 3 # only used if num_hist_steps: None
num_hist_steps_max: 21 #10 # only used if num_hist_steps: None
data_loading_params:
  batch_size: 1 
  num_workers: 6
gcn_parameters: ## the commented numbers are the values of the parameters in the original EGCN paper
  feats_per_node: 30 #50
  feats_per_node_min: 18 #30
  feats_per_node_max: 187 #312
  layer_1_feats: 46 #76
  layer_1_feats_min: 18 #30
  layer_1_feats_max: 300 #500
  layer_2_feats: None
  layer_2_feats_same_as_l1: True
  k_top_grcu: 200
  num_layers: 2
  lstm_l1_layers: 1
  lstm_l1_feats: 75 #125  # only used with sp_lstm_B_trainer
  lstm_l1_feats_min: 30 #50
  lstm_l1_feats_max: 300 #500
  lstm_l2_layers: 1 # only used with both sp_lstm_A_trainer and sp_lstm_B_trainer
  lstm_l2_feats: 240 #400 # only used with both sp_lstm_A_trainer and sp_lstm_B_trainer
  lstm_l2_feats_same_as_l1: True
  cls_feats: 306  #510 # Hidden size of the classifier
  cls_feats_min: 60 #100
  cls_feats_max: 420 #700
comments:
  - comments
